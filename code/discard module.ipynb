{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "151e6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "best_accuracy = 50\n",
    "#num_layers = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8eb3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71f4ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, chunk_size):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.columns = pd.read_csv(self.file_path, nrows=0).columns.str.strip().str.replace('\\xa0', ' ').tolist()\n",
    "        self.train_columns = [col for col in self.columns if col != 'discard']  # 排除 'discard' 欄位\n",
    "        self.num_samples = sum(1 for _ in open(self.file_path, encoding='utf-8')) - 1  # 計算總樣本數\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        chunk_start = idx // self.chunk_size * self.chunk_size\n",
    "        df = pd.read_csv(self.file_path, skiprows=chunk_start + 1, nrows=self.chunk_size, header=None, encoding='utf-8')  # 跳過標題和之前的行\n",
    "\n",
    "        df.columns = self.columns\n",
    "        \n",
    "        if 'discard' not in df.columns:\n",
    "            raise KeyError(f\"Chunk starting at row {chunk_start + 1} does not contain 'Discard' column.\")\n",
    "        \n",
    "            \n",
    "        sample_idx = idx % self.chunk_size\n",
    "        \n",
    "        train_data = df[self.train_columns].iloc[sample_idx].values.astype(\"float32\")\n",
    "        train_data = train_data.reshape(34, 29)\n",
    "        \n",
    "        value_data = df['discard'].iloc[sample_idx]  # 提取 'Discard' 欄位\n",
    "        \n",
    "        return torch.tensor(train_data, dtype=torch.float32), torch.tensor(value_data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de8770de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 34, 29])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "讀取csv\n",
    "\"\"\"\n",
    "file_path = 'E:/專題/data/2021/DiscardData.csv'\n",
    "dataset = MyDataset(file_path, chunk_size=50000)  # 每次只加載 50000 行\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 測試 DataLoader 是否正常運作\n",
    "for batch_features, batch_labels in train_dataloader:\n",
    "    print(batch_features.shape)  # 預期輸出: torch.Size([32, 34, 29])\n",
    "    print(batch_labels.shape)    # 預期輸出: torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fcfb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_layers = 10, input_features=34, input_channels=29, output_features=34):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=input_channels, out_channels=256, kernel_size=3, padding=1))\n",
    "        \n",
    "        for _ in range(1, num_layers - 1):\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=256, out_channels=1, kernel_size=1, padding=0))\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc = nn.Linear(input_features, output_features)\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0, 2, 1)  # 變成 (batch, 29, 34)\n",
    "\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            if(i < len(self.conv_layers) - 1):\n",
    "                x = self.activation(conv(x))\n",
    "            else:\n",
    "                x = conv(x)\n",
    "                \n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa607168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "127846be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "loss_criterion  = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44467e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17],\n",
      "       device='cuda:0')\n",
      "tensor([27, 30, 18, 22,  2, 23, 31,  8, 32, 25, 10, 21, 24,  3, 16, 17,  5, 16,\n",
      "        27,  8, 26, 32, 17, 18, 11, 14,  0, 22, 26,  7,  7,  4],\n",
      "       device='cuda:0')\n",
      "Correct Predictions: 2\n",
      "Total Predictions: 32\n",
      "Accuracy: 6.2500 %\n",
      "At epoch: 0, loss: 3.5316238403320312\n",
      "best accuracy: 50\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "last_saved_time = start_time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for train_data, value_data in train_dataloader:\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        channel1_tensor = train_data.unsqueeze(1)\n",
    "        channel1_tensor = channel1_tensor.to(device)\n",
    "        value_data = value_data.to(device)\n",
    "        \n",
    "        output = model(channel1_tensor)\n",
    "        output = output.float()\n",
    "        value_data = value_data.long()\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "        correct_predictions = (predicted_labels == value_data).sum().item()\n",
    "        accuracy = (correct_predictions / value_data.size(0))*100\n",
    "        \n",
    "        loss = loss_criterion(output, value_data)\n",
    "        \n",
    "        current_loss = loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output()\n",
    "\n",
    "        print(predicted_labels)\n",
    "        print(value_data)\n",
    "        print(f\"Correct Predictions: {correct_predictions}\")\n",
    "        print(f\"Total Predictions: {value_data.size(0)}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\",\"%\")\n",
    "        print(f\"At epoch: {epoch}, loss: {current_loss}\")\n",
    "        print(f\"best accuracy: {best_accuracy}\")\n",
    "        \n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model, f'E:/專題/discard_model/CNN_discard_model_{best_accuracy}.pth')\n",
    "            print(f'模型在 epoch {epoch} 之後被儲存，正確率: {best_accuracy}')\n",
    "        \n",
    "        current_time = time.time()\n",
    "        if current_time - last_saved_time >= 10800:\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime(current_time))\n",
    "            model_path = f'E:/專題/discard_model/CNN_discard_model_{timestamp}.pth'\n",
    "            torch.save(model, model_path)\n",
    "            last_saved_time = current_time\n",
    "            print(f'模型已基於時間間隔存儲，時間: {timestamp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
