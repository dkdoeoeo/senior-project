{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55a16cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "best_accuracy = 0\n",
    "#num_layers = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614a4911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timy3\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 13.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6995c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, chunk_size):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.columns = pd.read_csv(self.file_path, nrows=0).columns.str.strip().str.replace('\\xa0', ' ').tolist()\n",
    "        self.train_columns = [col for col in self.columns if col != 'discard']  # 排除 'discard' 欄位\n",
    "        self.num_samples = sum(1 for _ in open(self.file_path, encoding='utf-8')) - 1  # 計算總樣本數\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        chunk_start = idx // self.chunk_size * self.chunk_size\n",
    "        df = pd.read_csv(self.file_path, skiprows=chunk_start + 1, nrows=self.chunk_size, header=None, encoding='utf-8')  # 跳過標題和之前的行\n",
    "\n",
    "        df.columns = self.columns\n",
    "        \n",
    "        if 'discard' not in df.columns:\n",
    "            raise KeyError(f\"Chunk starting at row {chunk_start + 1} does not contain 'Discard' column.\")\n",
    "        \n",
    "            \n",
    "        sample_idx = idx % self.chunk_size\n",
    "        \n",
    "        train_data = df[self.train_columns].iloc[sample_idx].values.astype(\"float32\")\n",
    "        train_data = train_data.reshape(34, 29)\n",
    "        \n",
    "        value_data = df['discard'].iloc[sample_idx]  # 提取 'Discard' 欄位\n",
    "        \n",
    "        return torch.tensor(train_data, dtype=torch.float32), torch.tensor(value_data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e528e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 34, 29])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "讀取csv\n",
    "\"\"\"\n",
    "file_path = 'E:/專題/data/2021/DiscardData.csv'\n",
    "dataset = MyDataset(file_path, chunk_size=50000)  # 每次只加載 50000 行\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 測試 DataLoader 是否正常運作\n",
    "for batch_features, batch_labels in train_dataloader:\n",
    "    print(batch_features.shape)  # 預期輸出: torch.Size([32, 34, 29])\n",
    "    print(batch_labels.shape)    # 預期輸出: torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7aa1c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_layers = 10, input_features=34, input_channels=29, output_features=34):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=input_channels, out_channels=300, kernel_size=3, padding=1))\n",
    "        \n",
    "        for _ in range(1, num_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv1d(in_channels=300, out_channels=300, kernel_size=3, padding=1),\n",
    "            )\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=300, out_channels=1, kernel_size=1, padding=0))\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc = nn.Linear(input_features, output_features)\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            if(i < len(self.conv_layers) - 1):\n",
    "                x = self.activation(conv(x))\n",
    "            else:\n",
    "                x = conv(x)\n",
    "                \n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba23899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "021c3664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "loss_criterion  = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4916c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32, 28, 29, 26, 29, 28, 29, 27, 18, 27, 28, 28, 25, 28, 27,  8, 29, 28,\n",
      "         7,  8, 31, 27, 27,  8, 28, 31, 18, 28, 28, 31, 18, 28],\n",
      "       device='cuda:0')\n",
      "tensor([24, 32, 19, 30, 24, 26, 13,  0, 17, 18, 26, 18, 27,  7,  0,  3, 10,  6,\n",
      "        19, 18, 18, 10, 27, 19, 28, 29,  8,  7, 30, 27, 28, 29],\n",
      "       device='cuda:0')\n",
      "Correct Predictions: 2\n",
      "Total Predictions: 32\n",
      "Accuracy: 6.2500 %\n",
      "At epoch: 0, loss: 3.2115790843963623\n",
      "best accuracy: 31.25\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "last_saved_time = start_time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for train_data, value_data in train_dataloader:\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        channel1_tensor = train_data.unsqueeze(1)\n",
    "        channel1_tensor = channel1_tensor.to(device)\n",
    "        value_data = value_data.to(device)\n",
    "        \n",
    "        output = model(channel1_tensor)\n",
    "        output = output.float()\n",
    "        value_data = value_data.long()\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "        correct_predictions = (predicted_labels == value_data).sum().item()\n",
    "        accuracy = (correct_predictions / value_data.size(0))*100\n",
    "        \n",
    "        loss = loss_criterion(output, value_data)\n",
    "        \n",
    "        current_loss = loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output()\n",
    "\n",
    "        print(predicted_labels)\n",
    "        print(value_data)\n",
    "        print(f\"Correct Predictions: {correct_predictions}\")\n",
    "        print(f\"Total Predictions: {value_data.size(0)}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\",\"%\")\n",
    "        print(f\"At epoch: {epoch}, loss: {current_loss}\")\n",
    "        print(f\"best accuracy: {best_accuracy}\")\n",
    "        \n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model, f'E:/專題/discard_model/DiscardModel_300/CNN_discard_model_{best_accuracy}.pth')\n",
    "            print(f'模型在 epoch {epoch} 之後被儲存，正確率: {best_accuracy}')\n",
    "\n",
    "        current_time = time.time()\n",
    "        if current_time - last_saved_time >= 10800:\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime(current_time))\n",
    "            model_path = f'E:/專題/discard_model/DiscardModel_300/CNN_discard_model_{timestamp}.pth'\n",
    "            torch.save(model, model_path)\n",
    "            last_saved_time = current_time\n",
    "            print(f'模型已基於時間間隔存儲，時間: {timestamp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
