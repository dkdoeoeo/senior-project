{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151e6b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "best_accuracy = 0\n",
    "#num_layers = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8eb3b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\timy3\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 13.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71f4ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path, chunk_size):\n",
    "        self.file_path = file_path\n",
    "        self.chunk_size = chunk_size\n",
    "        \n",
    "        self.columns = pd.read_csv(self.file_path, nrows=0).columns.str.strip().str.replace('\\xa0', ' ').tolist()\n",
    "        self.train_columns = [col for col in self.columns if col != 'discard']  # 排除 'discard' 欄位\n",
    "        self.num_samples = sum(1 for _ in open(self.file_path, encoding='utf-8')) - 1  # 計算總樣本數\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        chunk_start = idx // self.chunk_size * self.chunk_size\n",
    "        df = pd.read_csv(self.file_path, skiprows=chunk_start + 1, nrows=self.chunk_size, header=None, encoding='utf-8')  # 跳過標題和之前的行\n",
    "\n",
    "        df.columns = self.columns\n",
    "        \n",
    "        if 'discard' not in df.columns:\n",
    "            raise KeyError(f\"Chunk starting at row {chunk_start + 1} does not contain 'Discard' column.\")\n",
    "        \n",
    "            \n",
    "        sample_idx = idx % self.chunk_size\n",
    "        \n",
    "        train_data = df[self.train_columns].iloc[sample_idx].values.astype(\"float32\")\n",
    "        train_data = train_data.reshape(34, 29)\n",
    "        \n",
    "        value_data = df['discard'].iloc[sample_idx]  # 提取 'Discard' 欄位\n",
    "        \n",
    "        return torch.tensor(train_data, dtype=torch.float32), torch.tensor(value_data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de8770de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 34, 29])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "讀取csv\n",
    "\"\"\"\n",
    "file_path = 'E:/專題/data/2021/DiscardData.csv'\n",
    "dataset = MyDataset(file_path, chunk_size=50000)  # 每次只加載 50000 行\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 測試 DataLoader 是否正常運作\n",
    "for batch_features, batch_labels in train_dataloader:\n",
    "    print(batch_features.shape)  # 預期輸出: torch.Size([32, 34, 29])\n",
    "    print(batch_labels.shape)    # 預期輸出: torch.Size([32])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcfb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(self, num_layers = 10, input_features=34, input_channels=29, output_features=34):\n",
    "        super(MyCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=input_channels, out_channels=512, kernel_size=3, padding=1))\n",
    "        \n",
    "        for _ in range(1, num_layers - 1):\n",
    "            self.conv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "                    nn.BatchNorm1d(512),\n",
    "                    nn.LeakyReLU(0.01)\n",
    "                    )\n",
    "            )\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=512, out_channels=1, kernel_size=1, padding=0))\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc = nn.Linear(input_features, output_features)\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            if i == 0:\n",
    "                x = self.activation(conv(x))\n",
    "            elif(i < len(self.conv_layers) - 1):\n",
    "                residual = x\n",
    "                out = conv(x)\n",
    "                x = self.activation(out + residual)\n",
    "            else:\n",
    "                x = conv(x)\n",
    "                \n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa607168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "127846be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "loss_criterion  = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44467e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32,  1, 17, 32, 33,  8,  9, 30, 33, 29, 30, 33, 27,  9, 30, 32, 31, 30,\n",
      "        28, 10, 28, 28, 26, 26, 30, 30, 26, 10, 30, 26, 26,  0],\n",
      "       device='cuda:0')\n",
      "tensor([ 0, 33, 11, 32, 29,  8,  8,  8,  0, 29, 17, 33, 32, 25, 15, 27, 31, 30,\n",
      "        33, 23,  9, 12, 17,  7, 27, 28,  0, 20, 21, 32,  4, 11],\n",
      "       device='cuda:0')\n",
      "Correct Predictions: 6\n",
      "Total Predictions: 32\n",
      "Accuracy: 18.7500 %\n",
      "At epoch: 0, loss: 3.054187059402466\n",
      "best accuracy: 37.5\n"
     ]
    }
   ],
   "source": [
    "current_loss = 0\n",
    "\n",
    "start_time = time.time()\n",
    "last_saved_time = start_time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for train_data, value_data in train_dataloader:\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        channel1_tensor = train_data.unsqueeze(1)\n",
    "        channel1_tensor = channel1_tensor.to(device)\n",
    "        value_data = value_data.to(device)\n",
    "        \n",
    "        output = model(channel1_tensor)\n",
    "        output = output.float()\n",
    "        value_data = value_data.long()\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_labels = torch.argmax(probabilities, dim=1)\n",
    "        correct_predictions = (predicted_labels == value_data).sum().item()\n",
    "        accuracy = (correct_predictions / value_data.size(0))*100\n",
    "        \n",
    "        loss = loss_criterion(output, value_data)\n",
    "        \n",
    "        current_loss = loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output()\n",
    "\n",
    "        print(predicted_labels)\n",
    "        print(value_data)\n",
    "        print(f\"Correct Predictions: {correct_predictions}\")\n",
    "        print(f\"Total Predictions: {value_data.size(0)}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\",\"%\")\n",
    "        print(f\"At epoch: {epoch}, loss: {current_loss}\")\n",
    "        print(f\"best accuracy: {best_accuracy}\")\n",
    "        \n",
    "        if accuracy >= best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model, f'E:/專題/discard_model/DiscardModel_512_Bat_Res/CNN_discard_model_{best_accuracy}.pth')\n",
    "            print(f'模型在 epoch {epoch} 之後被儲存，正確率: {best_accuracy}')\n",
    "\n",
    "        current_time = time.time()\n",
    "        if current_time - last_saved_time >= 10800:\n",
    "            timestamp = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime(current_time))\n",
    "            model_path = f'E:/專題/discard_model/DiscardModel_512_Bat_Res/CNN_discard_model_{timestamp}.pth'\n",
    "            torch.save(model, model_path)\n",
    "            last_saved_time = current_time\n",
    "            print(f'模型已基於時間間隔存儲，時間: {timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'E:/專題/discard_model/CNN_discard_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c94cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscardCNN(nn.Module):\n",
    "    def __init__(self, num_layers = 10, input_features=34, input_channels=29, output_features=34):\n",
    "        super(DiscardCNN, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=input_channels, out_channels=256, kernel_size=3, padding=1))\n",
    "        \n",
    "        for _ in range(1, num_layers - 1):\n",
    "            self.conv_layers.append(nn.Conv1d(in_channels=256, out_channels=256, kernel_size=3, padding=1))\n",
    "        \n",
    "        self.conv_layers.append(nn.Conv1d(in_channels=256, out_channels=1, kernel_size=1, padding=0))\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc = nn.Linear(input_features, output_features)\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        for i, conv in enumerate(self.conv_layers):\n",
    "            if(i < len(self.conv_layers) - 1):\n",
    "                x = self.activation(conv(x))\n",
    "            else:\n",
    "                x = conv(x)\n",
    "                \n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42236eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timy3\\AppData\\Local\\Temp\\ipykernel_87804\\1567854380.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"E:/專題/discard_model/CNN_discard_model_weights.pth\"))  # 載入之前訓練的權重\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DiscardCNN()  # 先初始化一個相同結構的模型\n",
    "model.load_state_dict(torch.load(\"E:/專題/discard_model/CNN_discard_model_weights.pth\"))  # 載入之前訓練的權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'E:/專題/discard_model/DiscardCNN.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
